{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "p0-YhEpP_Ds-"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtap63bYWEz0"
      },
      "source": [
        "# CS246 - Homework 1\n",
        "\n",
        "## Question 2\n",
        "\n",
        "### Association Rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqWG0wxDnqFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588afa9c-6341-46bd-ed6d-d038bbb1cdd2"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 8.0 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 43.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=2a451e612c055fe1eb0a963d3af02c3fdc6f5fa729beb9e426596b1a76f53152\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 155219 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYRqHUPMoDrR"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8YewevWqi66"
      },
      "source": [
        "Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VN9w0zTqRXv"
      },
      "source": [
        "id='1NOJZTHn9U1DvJB9eci_Oyd_yGZ_C-Cvu'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('browsing.txt')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdBcKlo_rcjz"
      },
      "source": [
        "###Evaluation of item sets: \n",
        "我们通常有如下几种手段来衡量item set的优劣。<br>\n",
        "1 $Confidence$ 用来衡量当basket中已经含有A的时候，包含B的概率。\n",
        "$$conf(A \\rightarrow B) = Pr(B|A)$$\n",
        "2 $lift$ 提升度指当销售一个物品时，另一个物品销售率会增加多少\n",
        "$$lift(A \\rightarrow B) = \\frac{conf(A \\rightarrow B)}{S(B)}$$where $S(B) = \\frac{Support(B)}{N}$ <br>\n",
        "3 $Conviction$ \n",
        "$$conv(A \\rightarrow B) = \\frac{1 - S(B)}{1 - conf(A \\rightarrow B)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_4lW24LmYVC"
      },
      "source": [
        "Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo1c66c0rUsh"
      },
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "from pyspark.mllib.fpm import FPGrowth\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCngWmwdreOF"
      },
      "source": [
        "data = sc.textFile(\"browsing.txt\")\n",
        "transactions = data.map(lambda line: list(set(line.strip().split(' '))))\n",
        "model = FPGrowth.train(transactions, minSupport= 100 / transactions.count(), numPartitions=10)\n",
        "result = model.freqItemsets()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53703I1CzqIo",
        "outputId": "143ec58c-26b4-40d7-e0e2-358dcdeacbd1"
      },
      "source": [
        "result.take(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FreqItemset(items=['SNA21851'], freq=120),\n",
              " FreqItemset(items=['DAI59609'], freq=350),\n",
              " FreqItemset(items=['GRO56989'], freq=655)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVO8VN9M3JAu"
      },
      "source": [
        "通过直接调用FP-Growth算法，我们可以直接得到dataset对应的frequent sets。下面的代码将完成associate rules的挖掘。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPAT-MhAuxjE",
        "outputId": "3e9e1570-d7e7-4f33-db98-63ee6e1b6b90"
      },
      "source": [
        "result.filter(lambda line: len(line[0]) == 1).count()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "647"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gr9o1MB3l0t"
      },
      "source": [
        "为了挖掘associate rules，我们定义如下的数据结构。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUNwVsfBNB9n"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "class CounterDict:\n",
        "    def __init__(self):\n",
        "        self.data = {}\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        if isinstance(key, Counter):\n",
        "            self.data[frozenset(key.items())] = value\n",
        "        else:\n",
        "            raise TypeError\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, Counter):\n",
        "            return self.data[frozenset(key.items())]\n",
        "        else:\n",
        "            raise TypeError\n",
        "\n",
        "def generate_hash_freq(freq_items):\n",
        "    h = CounterDict()\n",
        "    data = freq_items.map(lambda line: (line[0], line[1])).collect()\n",
        "    # line : （[item1,item2], cnt）\n",
        "    for line in data:\n",
        "        h[Counter(line[0])] = line[1]\n",
        "\n",
        "    return h\n",
        "\n",
        "\n",
        "def associate_rules(freq_items, h):\n",
        "    result = []\n",
        "    for line in freq_items:\n",
        "        for recommended in line[0]:\n",
        "          # line[0]包含了一个bucket中的所有items，我们遍历basket，从中挑出一个item作为recommended，basket中其余的items作为item_set\n",
        "          # 我们可以直接计算conf(recommended -> item_Set)\n",
        "            items_set = sorted(list(set(line[0]) - set([recommended])))\n",
        "            conf = h[Counter(line[0])] / h[Counter(items_set)] \n",
        "            result.append(((items_set, recommended), conf))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "h = generate_hash_freq(result)  # h : dict \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inlRhZam4UjL"
      },
      "source": [
        "我们首先filter所有二元的basket，即len(line[0]==2)，计算他们的conf。<br>\n",
        "接着筛选出所有的三元basket，选择一个作为recommended,其余作为itemsets，计算对应的conf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18gHN7NZnqq8",
        "outputId": "3b409c38-9762-4a06-fa0f-584e2e418e86"
      },
      "source": [
        "# {a} -> b\n",
        "pairs = result.filter(lambda line: len(line[0]) == 2).map(lambda line: ((line[0][0], line[0][1]), line[1]))\n",
        "# pairs : ((item1, item2), cnt)\n",
        "double_rules = associate_rules(pairs.collect(), h)\n",
        "sorted(double_rules, key=lambda tup: -tup[1])[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((['DAI93865'], 'FRO40251'), 1.0),\n",
              " ((['GRO85051'], 'FRO40251'), 0.999176276771005),\n",
              " ((['GRO38636'], 'FRO40251'), 0.9906542056074766),\n",
              " ((['ELE12951'], 'FRO40251'), 0.9905660377358491),\n",
              " ((['DAI88079'], 'FRO40251'), 0.9867256637168141)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdSannn6miw4",
        "outputId": "24f1cfa7-aa34-4909-b2e5-4a6b0b079fcd"
      },
      "source": [
        "# {a, b} -> c\n",
        "triples = result.filter(lambda line: len(line[0]) == 3).map(lambda line: ((line[0][0], line[0][1], line[0][2]), line[1]))\n",
        "triple_rules = associate_rules(triples.collect(), h)\n",
        "sorted(triple_rules, key=lambda tup: (-tup[1], tup[0]))[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((['DAI23334', 'ELE92920'], 'DAI62779'), 1.0),\n",
              " ((['DAI31081', 'GRO85051'], 'FRO40251'), 1.0),\n",
              " ((['DAI55911', 'GRO85051'], 'FRO40251'), 1.0),\n",
              " ((['DAI62779', 'DAI88079'], 'FRO40251'), 1.0),\n",
              " ((['DAI75645', 'GRO85051'], 'FRO40251'), 1.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr8z4abHmimX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDc7OxBnmiak"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}